{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db425a82",
   "metadata": {},
   "source": [
    "# Wind Downscaling\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Conda environment\n",
    "* Get a Copernicus API key from: https://cds.climate.copernicus.eu/api-how-to\n",
    "  * create a file at \\$HOME/.cdsapirc with the required UID and key\n",
    "* Create a .env file in the same folder as this notebook, and add the COSMO_USERNAME and COSMO_PASSWORD to connect to the UNI-BE server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5883d4ac",
   "metadata": {},
   "source": [
    "## Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711de2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536c3d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/Boubou/Documents/GitHub/WindDownscaling_EPFL_UNIBE\n",
      "Installing collected packages: downscaling\n",
      "  Attempting uninstall: downscaling\n",
      "    Found existing installation: downscaling 1.0\n",
      "    Uninstalling downscaling-1.0:\n",
      "      Successfully uninstalled downscaling-1.0\n",
      "  Running setup.py develop for downscaling\n",
      "Successfully installed downscaling-1.0\n"
     ]
    }
   ],
   "source": [
    "if Path('./setup.py').exists():\n",
    "    !pip install -e .\n",
    "else:\n",
    "    !pip install -U git+https://github.com/OpheliaMiralles/WindDownscaling_EPFL_UNIBE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ccaa2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.1\n",
      "  latest version: 4.10.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c conda-forge gdal tensorflow xarray numpy=1.19.5 pandas pysftp cdsapi elevation rasterio dask python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e91b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: topo-descriptors in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: pandas in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (1.3.2)\n",
      "Requirement already satisfied: utm in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (0.7.0)\n",
      "Requirement already satisfied: yaconfigobject in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (1.2.3)\n",
      "Requirement already satisfied: numpy in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (1.19.5)\n",
      "Requirement already satisfied: xarray in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (0.19.0)\n",
      "Requirement already satisfied: netcdf4 in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (1.5.6)\n",
      "Requirement already satisfied: scipy in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (1.6.2)\n",
      "Requirement already satisfied: cftime in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from netcdf4->topo-descriptors) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from pandas->topo-descriptors) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from pandas->topo-descriptors) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->topo-descriptors) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=40.4 in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from xarray->topo-descriptors) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pyyaml in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from yaconfigobject->topo-descriptors) (5.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install topo-descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46502d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks as cb\n",
    "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff71a3",
   "metadata": {},
   "source": [
    "## Set configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "029c1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('./data')\n",
    "ERA5_DATA_FOLDER = DATA_ROOT / 'ERA5'\n",
    "COSMO1_DATA_FOLDER = DATA_ROOT / 'COSMO1'\n",
    "DEM_DATA_FILE = DATA_ROOT / 'dem/Switzerland-90m-DEM.tif'\n",
    "PROCESSED_DATA_FOLDER = DATA_ROOT / 'img_prediction_files'\n",
    "\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "ERA5_DATA_FOLDER.mkdir(exist_ok=True)\n",
    "COSMO1_DATA_FOLDER.mkdir(exist_ok=True)\n",
    "DEM_DATA_FILE.parent.mkdir(exist_ok=True)\n",
    "PROCESSED_DATA_FOLDER.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c457e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERA5_PREDICTORS_SURFACE = ('u10', 'v10', 'blh', 'fsr', 'sp', 'sshf',\n",
    "                               'u100', 'v100')\n",
    "ERA5_PREDICTORS_Z500 = ('d', 'z', 'u', 'v', 'w', 'vo')\n",
    "TOPO_PREDICTORS = ('tpi_500', 'ridge_index_norm', 'ridge_index_dir',\n",
    "                   'we_derivative', 'sn_derivative',\n",
    "                   'slope', 'aspect')\n",
    "ALL_INPUTS = ERA5_PREDICTORS_SURFACE + ERA5_PREDICTORS_Z500 + TOPO_PREDICTORS\n",
    "ALL_INPUTS = ('u10', 'v10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ac9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start and end date for the data - should be in the 2016-2020 range\n",
    "START_DATE = date(2016,4,1)\n",
    "END_DATE = date(2018,4,1)\n",
    "NUM_DAYS = (END_DATE-START_DATE).days + 1\n",
    "# Number of consecutive images to form a sequence\n",
    "SEQUENCE_LENGTH = 6\n",
    "# Size of the high resolution image to be produced\n",
    "IMG_SIZE = 64\n",
    "# Number of noise channels to add to the image\n",
    "NOISE_CHANNELS = 4\n",
    "# Number of sequences per batch\n",
    "BATCH_SIZE = 16\n",
    "# Number of workers to run to process the data to create the batches\n",
    "BATCH_WORKERS = 8\n",
    "# Latent dimension for the autoencoder\n",
    "USE_AUTOENCODER = True\n",
    "AUTOENCODER_OUTPUT_FEATURES = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017821e",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d1ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.download_ERA5 import download_ERA5\n",
    "download_ERA5(ERA5_DATA_FOLDER, START_DATE, END_DATE)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11878bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEM_DATA_FILE.exists():\n",
    "    dest = str(DEM_DATA_FILE)\n",
    "    !eio --product SRTM3 clip -o {dest} --bounds 5.27 45.46 11.02 48.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_processing import process_topographic_variables_file\n",
    "process_topographic_variables_file(DEM_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ba12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv('COSMO_USERNAME')\n",
    "password = os.getenv('COSMO_PASSWORD')\n",
    "from data import download_COSMO1\n",
    "download_COSMO1(username, password, COSMO1_DATA_FOLDER, START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d70768",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from data.data_processing import process_imgs\n",
    "process_imgs(PROCESSED_DATA_FOLDER, ERA5_DATA_FOLDER, COSMO1_DATA_FOLDER, DEM_DATA_FILE.parent,\n",
    "             surface_variables_included=ERA5_PREDICTORS_SURFACE,\n",
    "             z500_variables_included=ERA5_PREDICTORS_Z500,\n",
    "             topo_variables_included=TOPO_PREDICTORS,\n",
    "             cosmo_variables_included=('U_10M', 'V_10M'),\n",
    "             start_date=START_DATE, end_date=END_DATE)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81801a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_generator import BatchGenerator, NaiveDecoder\n",
    "\n",
    "batch_gen = BatchGenerator(path_to_data=PROCESSED_DATA_FOLDER, decoder=NaiveDecoder(normalize=True),\n",
    "                           sequence_length=SEQUENCE_LENGTH,\n",
    "                           patch_length_pixel=IMG_SIZE, batch_size=BATCH_SIZE,\n",
    "                           input_variables=ALL_INPUTS,\n",
    "                           start_date=START_DATE, end_date=END_DATE,\n",
    "                           num_workers=BATCH_WORKERS)\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "with batch_gen as batch:\n",
    "    for b in range(NUM_DAYS):\n",
    "        print(f'Creating batch {b+1}/{NUM_DAYS}')\n",
    "        x, y = next(batch)\n",
    "        inputs.append(x)\n",
    "        outputs.append(y)\n",
    "inputs = np.concatenate(inputs, axis=0)\n",
    "outputs = np.concatenate(outputs, axis=0)\n",
    "print(f\"Inputs: {inputs.shape}\")\n",
    "print(f\"Outputs: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CHANNELS = len(ALL_INPUTS)\n",
    "if USE_AUTOENCODER:\n",
    "    checkpoint_path_weights = Path('./checkpoints/autoencoder/weights.ckpt')\n",
    "    if not checkpoint_path_weights.exists():\n",
    "        print(\"No autoencoder weights found!\")\n",
    "    else:\n",
    "        autoencoder = AutoEncoder(nb_channels_in=len(ALL_INPUTS), nb_channels_out=OUTPUT_FEATURES,\n",
    "                           time_steps=SEQUENCE_LENGTH, img_size=IMG_SIZE)\n",
    "        autoencoder.load_weights(checkpoint_path_weights)\n",
    "\n",
    "        print(\"Reducing data dimension\")\n",
    "        inputs = autoencoder.encoder.predict(inputs)\n",
    "        INPUT_CHANNELS = AUTOENCODER_OUTPUT_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d07769",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from data.data_generator import NoiseGenerator\n",
    "from gan import train\n",
    "from gan.models import make_generator, make_discriminator\n",
    "\n",
    "generator = make_generator(image_size=IMG_SIZE, in_channels=INPUT_CHANNELS,\n",
    "                           noise_channels=NOISE_CHANNELS, out_channels=2,\n",
    "                           n_timesteps=SEQUENCE_LENGTH)\n",
    "generator.compile(train.generator_optimizer())\n",
    "print(f\"Generator: {generator.count_params():,} weights\")\n",
    "\n",
    "discriminator = make_discriminator(low_res_size=IMG_SIZE, high_res_size=IMG_SIZE, low_res_channels=INPUT_CHANNELS,\n",
    "                                   high_res_channels=2, n_timesteps=SEQUENCE_LENGTH)\n",
    "discriminator.compile(train.discriminator_optimizer(), train.discriminator_loss)\n",
    "print(f\"Discriminator: {discriminator.count_params():,} weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af4265",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gan.ganbase import  GAN\n",
    "\n",
    "noise_shape = (BATCH_SIZE, SEQUENCE_LENGTH, IMG_SIZE, IMG_SIZE, NOISE_CHANNELS)\n",
    "gan = GAN(generator, discriminator, noise_generator=NoiseGenerator(noise_shape))\n",
    "\n",
    "print(f\"Total: {gan.generator.count_params() + gan.discriminator.count_params():,} weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76efc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_weights = Path('./checkpoints/gan/weights.ckpt')\n",
    "checkpoint_path_weights.parent.mkdir(exist_ok=True, parents=True)\n",
    "log_path = Path('./logs/gan')\n",
    "if log_path.exists():\n",
    "    log_path_str = str(log_path)\n",
    "    !rm -rf {log_path_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b753c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs/gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    cb.TensorBoard(log_path, write_images=True, histogram_freq=1, profile_batch=(2, 4)),\n",
    "    cb.ProgbarLogger('steps'),\n",
    "    cb.EarlyStopping(min_delta=2e-3, patience=10),\n",
    "    cb.TerminateOnNaN(),\n",
    "    cb.ModelCheckpoint(checkpoint_path_weights, monitor='loss', save_best_only=True, save_weights_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a9f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.fit(inputs, outputs, callbacks=callbacks, epochs=30, batch_size=BATCH_SIZE, validation_split=0.15, steps_per_epoch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = make_discriminator(low_res_size=IMG_SIZE, high_res_size=IMG_SIZE, low_res_channels=INPUT_CHANNELS,\n",
    "                                   high_res_channels=2, n_timesteps=SEQUENCE_LENGTH)\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_true * y_pred)\n",
    "    return train.discriminator_loss(y_true, y_pred)\n",
    "\n",
    "disc.compile(train.discriminator_optimizer(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = NoiseGenerator(np.shape(inputs))()\n",
    "fake_high_res = generator([inputs, noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4789b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = list(zip(inputs, outputs))\n",
    "inp += list(zip(inputs, fake_high_res))\n",
    "inp = [np.concatenate((inputs, inputs)), np.concatenate((outputs, fake_high_res))]\n",
    "print(inp[0][0].shape, inp[0][1].shape, len(inp[0]), len(inp[1]))\n",
    "labels = np.array([-np.ones((len(inputs), 1)), np.ones((len(inputs), 1))]).reshape(-1)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d588c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show(image, dims=2, legends=None):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for i in range(dims):\n",
    "        label = legends[i] if legends is not None else ''\n",
    "        plt.subplot(1, dims, i+1)\n",
    "        plt.imshow(image[0, :, :, i], cmap='jet')\n",
    "        plt.title(label)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "show(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f5290",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1695c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show(fake_high_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7894cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(disc.predict([inputs, outputs])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99166e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(disc.predict([inputs, fake_high_res])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82478ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.fit(inp, labels, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.predict([inputs[:2], outputs[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5acc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(disc.predict([inputs, fake_high_res])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f3e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = disc.evaluate(inputs, outputs)\n",
    "print(evals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:downscale_dev]",
   "language": "python",
   "name": "conda-env-downscale_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
