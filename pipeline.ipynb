{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db425a82",
   "metadata": {},
   "source": [
    "# Wind Downscaling\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Conda environment\n",
    "* Get a Copernicus API key from: https://cds.climate.copernicus.eu/api-how-to\n",
    "  * create a file at \\$HOME/.cdsapirc with the required UID and key\n",
    "* Create a .env file in the same folder as this notebook, and add the COSMO_USERNAME and COSMO_PASSWORD to connect to the UNI-BE server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5883d4ac",
   "metadata": {},
   "source": [
    "## Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711de2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536c3d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/Boubou/Documents/GitHub/WindDownscaling_EPFL_UNIBE\n",
      "Installing collected packages: downscaling\n",
      "  Attempting uninstall: downscaling\n",
      "    Found existing installation: downscaling 1.0\n",
      "    Uninstalling downscaling-1.0:\n",
      "      Successfully uninstalled downscaling-1.0\n",
      "  Running setup.py develop for downscaling\n",
      "Successfully installed downscaling-1.0\n"
     ]
    }
   ],
   "source": [
    "if Path('./setup.py').exists():\n",
    "    !pip install -e .\n",
    "else:\n",
    "    !pip install -U git+https://github.com/OpheliaMiralles/WindDownscaling_EPFL_UNIBE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ccaa2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.1\n",
      "  latest version: 4.10.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c conda-forge gdal tensorflow xarray numpy=1.19.5 pandas pysftp cdsapi elevation rasterio dask python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e91b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: topo-descriptors in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: netcdf4 in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (1.5.6)\n",
      "Requirement already satisfied: xarray in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (0.19.0)\n",
      "Requirement already satisfied: yaconfigobject in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (1.2.3)\n",
      "Requirement already satisfied: pandas in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (1.3.3)\n",
      "Requirement already satisfied: numpy in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (1.19.5)\n",
      "Requirement already satisfied: utm in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (0.7.0)\n",
      "Requirement already satisfied: scipy in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from topo-descriptors) (1.6.2)\n",
      "Requirement already satisfied: cftime in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from netcdf4->topo-descriptors) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from pandas->topo-descriptors) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from pandas->topo-descriptors) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->topo-descriptors) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=40.4 in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from xarray->topo-descriptors) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pyyaml in /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages (from yaconfigobject->topo-descriptors) (5.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install topo-descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46502d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks as cb\n",
    "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff71a3",
   "metadata": {},
   "source": [
    "## Set configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "029c1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('./data')\n",
    "ERA5_DATA_FOLDER = DATA_ROOT / 'ERA5'\n",
    "COSMO1_DATA_FOLDER = DATA_ROOT / 'COSMO1'\n",
    "DEM_DATA_FILE = DATA_ROOT / 'dem/Switzerland-90m-DEM.tif'\n",
    "PROCESSED_DATA_FOLDER = DATA_ROOT / 'img_prediction_files'\n",
    "\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "ERA5_DATA_FOLDER.mkdir(exist_ok=True)\n",
    "COSMO1_DATA_FOLDER.mkdir(exist_ok=True)\n",
    "DEM_DATA_FILE.parent.mkdir(exist_ok=True)\n",
    "PROCESSED_DATA_FOLDER.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c457e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERA5_PREDICTORS_SURFACE = ('u10', 'v10', 'blh', 'fsr', 'sp', 'sshf',\n",
    "                               'u100', 'v100')\n",
    "ERA5_PREDICTORS_Z500 = ('d', 'z', 'u', 'v', 'w', 'vo')\n",
    "TOPO_PREDICTORS = ('tpi_500', 'ridge_index_norm', 'ridge_index_dir',\n",
    "                   'we_derivative', 'sn_derivative',\n",
    "                   'slope', 'aspect')\n",
    "ALL_INPUTS = ['u10', 'sp']+ list(TOPO_PREDICTORS) #ERA5_PREDICTORS_SURFACE + TOPO_PREDICTORS\n",
    "ALL_OUTPUTS = ['U_10M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ac9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start and end date for the data - should be in the 2016-2020 range\n",
    "START_DATE = date(2016,4,1)\n",
    "END_DATE = date(2016,6,1)\n",
    "NUM_DAYS = (END_DATE-START_DATE).days + 1\n",
    "# Number of consecutive images to form a sequence\n",
    "SEQUENCE_LENGTH = 3\n",
    "# Size of the high resolution image to be produced\n",
    "IMG_SIZE = 128\n",
    "# Number of noise channels to add to the image\n",
    "NOISE_CHANNELS = 1000\n",
    "# Number of sequences per batch\n",
    "BATCH_SIZE = 8\n",
    "# Number of workers to run to process the data to create the batches\n",
    "BATCH_WORKERS = 8\n",
    "# Latent dimension for the autoencoder\n",
    "USE_AUTOENCODER = True\n",
    "AUTOENCODER_OUTPUT_FEATURES = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017821e",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "918d1ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20160401_era5_surface_hourly already exists\n",
      "File 20160402_era5_surface_hourly already exists\n",
      "File 20160403_era5_surface_hourly already exists\n",
      "File 20160404_era5_surface_hourly already exists\n",
      "File 20160405_era5_surface_hourly already exists\n",
      "File 20160406_era5_surface_hourly already exists\n",
      "File 20160407_era5_surface_hourly already exists\n",
      "File 20160408_era5_surface_hourly already exists\n",
      "File 20160409_era5_surface_hourly already exists\n",
      "File 20160410_era5_surface_hourly already exists\n",
      "File 20160411_era5_surface_hourly already exists\n",
      "File 20160412_era5_surface_hourly already exists\n",
      "File 20160413_era5_surface_hourly already exists\n",
      "File 20160414_era5_surface_hourly already exists\n",
      "File 20160415_era5_surface_hourly already exists\n",
      "File 20160416_era5_surface_hourly already exists\n",
      "File 20160417_era5_surface_hourly already exists\n",
      "File 20160418_era5_surface_hourly already exists\n",
      "File 20160419_era5_surface_hourly already exists\n",
      "File 20160420_era5_surface_hourly already exists\n",
      "File 20160421_era5_surface_hourly already exists\n",
      "File 20160422_era5_surface_hourly already exists\n",
      "File 20160423_era5_surface_hourly already exists\n",
      "File 20160424_era5_surface_hourly already exists\n",
      "File 20160425_era5_surface_hourly already exists\n",
      "File 20160426_era5_surface_hourly already exists\n",
      "File 20160427_era5_surface_hourly already exists\n",
      "File 20160428_era5_surface_hourly already exists\n",
      "File 20160429_era5_surface_hourly already exists\n",
      "File 20160430_era5_surface_hourly already exists\n",
      "File 20160501_era5_surface_hourly already exists\n",
      "File 20160502_era5_surface_hourly already exists\n",
      "File 20160503_era5_surface_hourly already exists\n",
      "File 20160504_era5_surface_hourly already exists\n",
      "File 20160505_era5_surface_hourly already exists\n",
      "File 20160506_era5_surface_hourly already exists\n",
      "File 20160507_era5_surface_hourly already exists\n",
      "File 20160508_era5_surface_hourly already exists\n",
      "File 20160509_era5_surface_hourly already exists\n",
      "File 20160510_era5_surface_hourly already exists\n",
      "File 20160511_era5_surface_hourly already exists\n",
      "File 20160512_era5_surface_hourly already exists\n",
      "File 20160513_era5_surface_hourly already exists\n",
      "File 20160514_era5_surface_hourly already exists\n",
      "File 20160515_era5_surface_hourly already exists\n",
      "File 20160516_era5_surface_hourly already exists\n",
      "File 20160517_era5_surface_hourly already exists\n",
      "File 20160518_era5_surface_hourly already exists\n",
      "File 20160519_era5_surface_hourly already exists\n",
      "File 20160520_era5_surface_hourly already exists\n",
      "File 20160521_era5_surface_hourly already exists\n",
      "File 20160522_era5_surface_hourly already exists\n",
      "File 20160523_era5_surface_hourly already exists\n",
      "File 20160524_era5_surface_hourly already exists\n",
      "File 20160525_era5_surface_hourly already exists\n",
      "File 20160526_era5_surface_hourly already exists\n",
      "File 20160527_era5_surface_hourly already exists\n",
      "File 20160528_era5_surface_hourly already exists\n",
      "File 20160529_era5_surface_hourly already exists\n",
      "File 20160530_era5_surface_hourly already exists\n",
      "File 20160531_era5_surface_hourly already exists\n",
      "File 20160601_era5_surface_hourly already exists\n",
      "File 20160401_era5_z500_hourly already exists\n",
      "File 20160402_era5_z500_hourly already exists\n",
      "File 20160403_era5_z500_hourly already exists\n",
      "File 20160404_era5_z500_hourly already exists\n",
      "File 20160405_era5_z500_hourly already exists\n",
      "File 20160406_era5_z500_hourly already exists\n",
      "File 20160407_era5_z500_hourly already exists\n",
      "File 20160408_era5_z500_hourly already exists\n",
      "File 20160409_era5_z500_hourly already exists\n",
      "File 20160410_era5_z500_hourly already exists\n",
      "File 20160411_era5_z500_hourly already exists\n",
      "File 20160412_era5_z500_hourly already exists\n",
      "File 20160413_era5_z500_hourly already exists\n",
      "File 20160414_era5_z500_hourly already exists\n",
      "File 20160415_era5_z500_hourly already exists\n",
      "File 20160416_era5_z500_hourly already exists\n",
      "File 20160417_era5_z500_hourly already exists\n",
      "File 20160418_era5_z500_hourly already exists\n",
      "File 20160419_era5_z500_hourly already exists\n",
      "File 20160420_era5_z500_hourly already exists\n",
      "File 20160421_era5_z500_hourly already exists\n",
      "File 20160422_era5_z500_hourly already exists\n",
      "File 20160423_era5_z500_hourly already exists\n",
      "File 20160424_era5_z500_hourly already exists\n",
      "File 20160425_era5_z500_hourly already exists\n",
      "File 20160426_era5_z500_hourly already exists\n",
      "File 20160427_era5_z500_hourly already exists\n",
      "File 20160428_era5_z500_hourly already exists\n",
      "File 20160429_era5_z500_hourly already exists\n",
      "File 20160430_era5_z500_hourly already exists\n",
      "File 20160501_era5_z500_hourly already exists\n",
      "File 20160502_era5_z500_hourly already exists\n",
      "File 20160503_era5_z500_hourly already exists\n",
      "File 20160504_era5_z500_hourly already exists\n",
      "File 20160505_era5_z500_hourly already exists\n",
      "File 20160506_era5_z500_hourly already exists\n",
      "File 20160507_era5_z500_hourly already exists\n",
      "File 20160508_era5_z500_hourly already exists\n",
      "File 20160509_era5_z500_hourly already exists\n",
      "File 20160510_era5_z500_hourly already exists\n",
      "File 20160511_era5_z500_hourly already exists\n",
      "File 20160512_era5_z500_hourly already exists\n",
      "File 20160513_era5_z500_hourly already exists\n",
      "File 20160514_era5_z500_hourly already exists\n",
      "File 20160515_era5_z500_hourly already exists\n",
      "File 20160516_era5_z500_hourly already exists\n",
      "File 20160517_era5_z500_hourly already exists\n",
      "File 20160518_era5_z500_hourly already exists\n",
      "File 20160519_era5_z500_hourly already exists\n",
      "File 20160520_era5_z500_hourly already exists\n",
      "File 20160521_era5_z500_hourly already exists\n",
      "File 20160522_era5_z500_hourly already exists\n",
      "File 20160523_era5_z500_hourly already exists\n",
      "File 20160524_era5_z500_hourly already exists\n",
      "File 20160525_era5_z500_hourly already exists\n",
      "File 20160526_era5_z500_hourly already exists\n",
      "File 20160527_era5_z500_hourly already exists\n",
      "File 20160528_era5_z500_hourly already exists\n",
      "File 20160529_era5_z500_hourly already exists\n",
      "File 20160530_era5_z500_hourly already exists\n",
      "File 20160531_era5_z500_hourly already exists\n",
      "File 20160601_era5_z500_hourly already exists\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from data.download_ERA5 import download_ERA5\n",
    "download_ERA5(ERA5_DATA_FOLDER, START_DATE, END_DATE)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11878bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEM_DATA_FILE.exists():\n",
    "    dest = str(DEM_DATA_FILE)\n",
    "    !eio --product SRTM3 clip -o {dest} --bounds 5.27 45.46 11.02 48.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5de802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-21 15:20:41,221 INFO Loading /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages/topo_descriptors/config/topo_descriptors.conf.\n",
      "2021-09-21 15:20:41,222 INFO Loading configuration file: /Users/Boubou/opt/anaconda3/envs/downscale_dev/lib/python3.9/site-packages/topo_descriptors/config/topo_descriptors.conf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed all topo files\n"
     ]
    }
   ],
   "source": [
    "from data.data_processing import process_topographic_variables_file\n",
    "process_topographic_variables_file(DEM_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "892ba12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading COSMO data\n"
     ]
    }
   ],
   "source": [
    "username = os.getenv('COSMO_USERNAME')\n",
    "password = os.getenv('COSMO_PASSWORD')\n",
    "from data import download_COSMO1\n",
    "download_COSMO1(username, password, COSMO1_DATA_FOLDER, START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9d70768",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DEM data files\n",
      "Inputs and outputs for date 20160401 have already been processed.\n",
      "Inputs and outputs for date 20160402 have already been processed.\n",
      "Inputs and outputs for date 20160403 have already been processed.\n",
      "Inputs and outputs for date 20160404 have already been processed.\n",
      "Inputs and outputs for date 20160405 have already been processed.\n",
      "Inputs and outputs for date 20160406 have already been processed.\n",
      "Inputs and outputs for date 20160407 have already been processed.\n",
      "Inputs and outputs for date 20160408 have already been processed.\n",
      "Inputs and outputs for date 20160409 have already been processed.\n",
      "Inputs and outputs for date 20160410 have already been processed.\n",
      "Inputs and outputs for date 20160411 have already been processed.\n",
      "Inputs and outputs for date 20160412 have already been processed.\n",
      "Inputs and outputs for date 20160413 have already been processed.\n",
      "Inputs and outputs for date 20160414 have already been processed.\n",
      "Inputs and outputs for date 20160415 have already been processed.\n",
      "Inputs and outputs for date 20160416 have already been processed.\n",
      "Inputs and outputs for date 20160417 have already been processed.\n",
      "Inputs and outputs for date 20160418 have already been processed.\n",
      "Inputs and outputs for date 20160419 have already been processed.\n",
      "Inputs and outputs for date 20160420 have already been processed.\n",
      "Inputs and outputs for date 20160421 have already been processed.\n",
      "Inputs and outputs for date 20160422 have already been processed.\n",
      "Inputs and outputs for date 20160423 have already been processed.\n",
      "Inputs and outputs for date 20160424 have already been processed.\n",
      "Inputs and outputs for date 20160425 have already been processed.\n",
      "Inputs and outputs for date 20160426 have already been processed.\n",
      "Inputs and outputs for date 20160427 have already been processed.\n",
      "Inputs and outputs for date 20160428 have already been processed.\n",
      "Inputs and outputs for date 20160429 have already been processed.\n",
      "Inputs and outputs for date 20160430 have already been processed.\n",
      "Inputs and outputs for date 20160501 have already been processed.\n",
      "Inputs and outputs for date 20160502 have already been processed.\n",
      "Inputs and outputs for date 20160503 have already been processed.\n",
      "Inputs and outputs for date 20160504 have already been processed.\n",
      "Inputs and outputs for date 20160505 have already been processed.\n",
      "Inputs and outputs for date 20160506 have already been processed.\n",
      "Inputs and outputs for date 20160507 have already been processed.\n",
      "Inputs and outputs for date 20160508 have already been processed.\n",
      "Inputs and outputs for date 20160509 have already been processed.\n",
      "Inputs and outputs for date 20160510 have already been processed.\n",
      "Inputs and outputs for date 20160511 have already been processed.\n",
      "Inputs and outputs for date 20160512 have already been processed.\n",
      "Inputs and outputs for date 20160513 have already been processed.\n",
      "Inputs and outputs for date 20160514 have already been processed.\n",
      "Inputs and outputs for date 20160515 have already been processed.\n",
      "Inputs and outputs for date 20160516 have already been processed.\n",
      "Inputs and outputs for date 20160517 have already been processed.\n",
      "Inputs and outputs for date 20160518 have already been processed.\n",
      "Inputs and outputs for date 20160519 have already been processed.\n",
      "Inputs and outputs for date 20160520 have already been processed.\n",
      "Inputs and outputs for date 20160521 have already been processed.\n",
      "Inputs and outputs for date 20160522 have already been processed.\n",
      "Inputs and outputs for date 20160523 have already been processed.\n",
      "Inputs and outputs for date 20160524 have already been processed.\n",
      "Inputs and outputs for date 20160525 have already been processed.\n",
      "Inputs and outputs for date 20160526 have already been processed.\n",
      "Inputs and outputs for date 20160527 have already been processed.\n",
      "Inputs and outputs for date 20160528 have already been processed.\n",
      "Inputs and outputs for date 20160529 have already been processed.\n",
      "Inputs and outputs for date 20160530 have already been processed.\n",
      "Inputs and outputs for date 20160531 have already been processed.\n",
      "Inputs and outputs for date 20160601 have already been processed.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from data.data_processing import process_imgs\n",
    "process_imgs(PROCESSED_DATA_FOLDER, ERA5_DATA_FOLDER, COSMO1_DATA_FOLDER, DEM_DATA_FILE.parent,\n",
    "             surface_variables_included=ERA5_PREDICTORS_SURFACE,\n",
    "             z500_variables_included=ERA5_PREDICTORS_Z500,\n",
    "             topo_variables_included=TOPO_PREDICTORS,\n",
    "             cosmo_variables_included=('U_10M', 'V_10M'),\n",
    "             start_date=START_DATE, end_date=END_DATE)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81801a02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "Creating batch 1/62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-21 15:20:46,945 WARNING multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batch 2/62\n",
      "Creating batch 3/62\n",
      "Creating batch 4/62\n",
      "Creating batch 5/62\n",
      "Creating batch 6/62\n",
      "Creating batch 7/62\n",
      "Creating batch 8/62\n",
      "Creating batch 9/62\n",
      "Creating batch 10/62\n",
      "Creating batch 11/62\n",
      "Creating batch 12/62\n",
      "Creating batch 13/62\n",
      "Creating batch 14/62\n",
      "Creating batch 15/62\n",
      "Creating batch 16/62\n",
      "Creating batch 17/62\n",
      "Creating batch 18/62\n",
      "Creating batch 19/62\n",
      "Creating batch 20/62\n",
      "Creating batch 21/62\n",
      "Creating batch 22/62\n",
      "Creating batch 23/62\n",
      "Creating batch 24/62\n",
      "Creating batch 25/62\n",
      "Creating batch 26/62\n",
      "Creating batch 27/62\n",
      "Creating batch 28/62\n",
      "Creating batch 29/62\n",
      "Creating batch 30/62\n",
      "Creating batch 31/62\n",
      "Creating batch 32/62\n",
      "Creating batch 33/62\n",
      "Creating batch 34/62\n",
      "Creating batch 35/62\n",
      "Creating batch 36/62\n",
      "Creating batch 37/62\n",
      "Creating batch 38/62\n",
      "Creating batch 39/62\n",
      "Creating batch 40/62\n",
      "Creating batch 41/62\n",
      "Creating batch 42/62\n",
      "Creating batch 43/62\n",
      "Creating batch 44/62\n",
      "Creating batch 45/62\n",
      "Creating batch 46/62\n",
      "Creating batch 47/62\n",
      "Creating batch 48/62\n",
      "Creating batch 49/62\n",
      "Creating batch 50/62\n",
      "Creating batch 51/62\n",
      "Creating batch 52/62\n",
      "Creating batch 53/62\n",
      "Creating batch 54/62\n",
      "Creating batch 55/62\n",
      "Creating batch 56/62\n",
      "Creating batch 57/62\n",
      "Creating batch 58/62\n",
      "Creating batch 59/62\n",
      "Creating batch 60/62\n",
      "Creating batch 61/62\n",
      "Creating batch 62/62\n",
      "Inputs: (496, 3, 128, 128, 9)\n",
      "Outputs: (496, 3, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "from data.data_generator import BatchGenerator, NaiveDecoder\n",
    "\n",
    "batch_gen = BatchGenerator(path_to_data=PROCESSED_DATA_FOLDER, decoder=NaiveDecoder(normalize=True),\n",
    "                           sequence_length=SEQUENCE_LENGTH,\n",
    "                           patch_length_pixel=IMG_SIZE, batch_size=BATCH_SIZE,\n",
    "                           input_variables=ALL_INPUTS,\n",
    "                           output_variables= ALL_OUTPUTS,\n",
    "                           start_date=START_DATE, end_date=END_DATE,\n",
    "                           num_workers=BATCH_WORKERS)\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "with batch_gen as batch:\n",
    "    for b in range(NUM_DAYS):\n",
    "        print(f'Creating batch {b+1}/{NUM_DAYS}')\n",
    "        x, y = next(batch)\n",
    "        inputs.append(x)\n",
    "        outputs.append(y)\n",
    "inputs = np.concatenate(inputs, axis=0)\n",
    "outputs = np.concatenate(outputs, axis=0)\n",
    "print(f\"Inputs: {inputs.shape}\")\n",
    "print(f\"Outputs: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00ce7c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No autoencoder weights found!\n"
     ]
    }
   ],
   "source": [
    "INPUT_CHANNELS = len(ALL_INPUTS)\n",
    "OUT_CHANNELS = len(ALL_OUTPUTS)\n",
    "\n",
    "if USE_AUTOENCODER:\n",
    "    checkpoint_path_weights = Path('./checkpoints/autoencoder/weights.ckpt')\n",
    "    if not checkpoint_path_weights.exists():\n",
    "        print(\"No autoencoder weights found!\")\n",
    "    else:\n",
    "        autoencoder = AutoEncoder(nb_channels_in=len(ALL_INPUTS), nb_channels_out=OUTPUT_FEATURES,\n",
    "                           time_steps=SEQUENCE_LENGTH, img_size=IMG_SIZE)\n",
    "        autoencoder.load_weights(checkpoint_path_weights)\n",
    "\n",
    "        print(\"Reducing data dimension\")\n",
    "        inputs = autoencoder.encoder.predict(inputs)\n",
    "        INPUT_CHANNELS = AUTOENCODER_OUTPUT_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9db10307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#if 'gan.train' in sys.modules:\n",
    " #   del sys.modules['gan.train']\n",
    "#from gan import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4d07769",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator: 2,600,513 weights\n",
      "Discriminator: 36,361,309 weights\n"
     ]
    }
   ],
   "source": [
    "from gan import train, metrics\n",
    "from gan.models import make_generator, make_discriminator\n",
    "generator = make_generator(image_size=IMG_SIZE, in_channels=INPUT_CHANNELS,\n",
    "                           noise_channels=NOISE_CHANNELS, out_channels=OUT_CHANNELS,\n",
    "                           n_timesteps=SEQUENCE_LENGTH)\n",
    "generator.compile(train.generator_optimizer())\n",
    "print(f\"Generator: {generator.count_params():,} weights\")\n",
    "\n",
    "discriminator = make_discriminator(low_res_size=IMG_SIZE, high_res_size=IMG_SIZE, low_res_channels=INPUT_CHANNELS,\n",
    "                                   high_res_channels=OUT_CHANNELS, n_timesteps=SEQUENCE_LENGTH)\n",
    "discriminator.compile(train.discriminator_optimizer(), train.discriminator_loss)\n",
    "print(f\"Discriminator: {discriminator.count_params():,} weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88af4265",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 38,961,822 weights\n"
     ]
    }
   ],
   "source": [
    "from gan.ganbase import  GAN\n",
    "from data.data_generator import FlexibleNoiseGenerator\n",
    "\n",
    "noise_shape = (BATCH_SIZE, SEQUENCE_LENGTH, IMG_SIZE, IMG_SIZE, NOISE_CHANNELS)\n",
    "gan = GAN(generator, discriminator, noise_generator=FlexibleNoiseGenerator(noise_shape, std=1))\n",
    "\n",
    "print(f\"Total: {gan.generator.count_params() + gan.discriminator.count_params():,} weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b225e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(generator_optimizer=train.generator_optimizer(),\n",
    "            generator_metrics= [tf.keras.metrics.RootMeanSquaredError(), metrics.LogSpectralDistance(),\n",
    "                                metrics.WeightedRMSEForExtremes()],\n",
    "            discriminator_optimizer=train.discriminator_optimizer(),\n",
    "                discriminator_loss=train.discriminator_loss,\n",
    "           metrics = [metrics.discriminator_score_fake(), metrics.discriminator_score_real()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e76efc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_weights = Path('./checkpoints/gan/weights-{epoch:02d}.ckpt')\n",
    "checkpoint_path_weights.parent.mkdir(exist_ok=True, parents=True)\n",
    "log_path = Path('./logs/gan')\n",
    "if log_path.exists():\n",
    "    log_path_str = str(log_path)\n",
    "    !rm -rf {log_path_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b753c15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1371ecdf1a0eaad5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1371ecdf1a0eaad5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs/gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffa60c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show(images, dims=1, legends=None):\n",
    "    fig, axes = plt.subplots(ncols= len(images), figsize=(10, 10))\n",
    "    for ax, im in zip(axes, images):\n",
    "        for i in range(dims):\n",
    "            label = legends[i] if legends is not None else ''\n",
    "            ax.imshow(im[0, :, :, i], cmap='jet')\n",
    "            ax.set_title(label)\n",
    "            ax.axis('off')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a10ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dims):\n",
    "        self.dims = dims\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        noise = FlexibleNoiseGenerator(noise_shape)\n",
    "        show(self.model.generator([inputs[:self.dims], noise(self.dims)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "375c2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    cb.TensorBoard(log_path, write_images=True, update_freq='batch', profile_batch=(2, 4)),\n",
    "    cb.ProgbarLogger('steps'),\n",
    "    cb.TerminateOnNaN(),\n",
    "    ShowCallback(10),\n",
    "    cb.ModelCheckpoint(str(checkpoint_path_weights), monitor='root_mean_squared_error', mode='min',\n",
    "                       save_best_only=False, save_weights_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a9f11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-971d8efb5882>:10: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/47 [..............................] - ETA: 27:23 - d_loss: 0.0016 - g_loss: 1.0229e-05 - d_fake: 1.0229e-05 - d_real: 0.0016 - g_root_mean_squared_error: 2.6630 - g_lsd: 0.0000e+00 - g_extreme_rmse: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "gan.fit(inputs, outputs, callbacks=callbacks, epochs=1, batch_size=BATCH_SIZE, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "noise = FlexibleNoiseGenerator(noise_shape)()\n",
    "results = gan.generator.predict([inputs[i:i+BATCH_SIZE], noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f112fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in [7,14,35,38,42,50]:\n",
    "    gan.load_weights(f'checkpoints/gan/weights-{epoch:02d}.ckpt')\n",
    "    noise = FlexibleNoiseGenerator(noise_shape)()\n",
    "    #os.mkdir(f'plots/gan_pred/{epoch}')\n",
    "    for i in range(0, BATCH_SIZE*6, BATCH_SIZE):\n",
    "        results = gan.generator.predict([inputs[i:i+BATCH_SIZE], noise])\n",
    "        j=0\n",
    "        for inp, out, res in zip(inputs[i:i+BATCH_SIZE], outputs[i:i+BATCH_SIZE], results):\n",
    "            fig = show([inp, out, res])\n",
    "            #fig.savefig(f'plots/gan_pred/{epoch}/inp_{i}_batch_{j}.png')\n",
    "            j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe30950",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=1\n",
    "\n",
    "gan.load_weights(f'checkpoints/gan/weights-{epoch:02d}.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2123696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:downscale_dev]",
   "language": "python",
   "name": "conda-env-downscale_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
